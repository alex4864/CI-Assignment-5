\documentclass[a4paper]{article}
\usepackage{amsmath, amssymb, bm}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
	\begin{titlepage}
		\centering
		{\huge \bf Assignment 5\par}
		\vspace{1cm}
		{\Large Computational Intelligence, SS2018\par}
		\vspace{1cm}
		\begin{tabular}{|l|l|l|}
			\hline
			\multicolumn{3}{|c|}{\textbf{Team Members}}   \\ \hline
			Last name & First name & Matriculation Number \\ \hline
			Lee       & Eunseo     & 11739623             \\ \hline
			Shadley   & Alex       & 11739595             \\ \hline
			Lee       & Dayeong    & 11739321             \\ \hline
		\end{tabular}
	\end{titlepage}

	\section{Classiﬁcation/ Clustering}
	\subsection{2 dimensional feature}
	\subsubsection{Perform all of the above-mentioned tasks for the EM algorithm.}
	In the process of initializing the parameters, we used random function to select m points for calculating means. Therefore, the result is different for each process. We selected the best results from 10 trials.
	\begin{itemize}
	\item Compare the result with the labeled data set (i.e., consider labels as well). Make a scatter plot of the data and plot the Gaussian mixture model over this plot.

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{number_diff.png}
			\caption{label data}
		\end{center}
	\end{figure}
	First array is EM algorithm classification. Second array is answer classification.\\
	Three points are mis-classified.

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{gauss.png}
			\caption{Three gaussian with scatter data points}
		\end{center}
	\end{figure}

	EM algorithm suceeded to find three gaussian model and classified the data points well.

	\clearpage
	\item For your tests, select the correct number of components (K = 3), but also check the result when you use more or less components. How do you choose your initialization $\theta$0? Does this choice have an inﬂuence on the result

	When the number of components is 2, the result is the following figure.
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.4\textwidth]{K2.png}
			\caption{K $=$ 2}
		\end{center}
	\end{figure}

	When the number of components is 4, the result is the following figure.
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.4\textwidth]{K4.png}
			\caption{K $=$ 4}
		\end{center}
	\end{figure}

	For initialization $\theta0$, we refered to the class pdf. The following figure is the reference class pdf.

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{ref1.png}
			\includegraphics[width=0.5\textwidth]{ref2.png}
			\caption{The initialization process in the class pdf}
		\end{center}
	\end{figure}

	The initialization process influences the EM algorithm result. When the randomly selected points, for calculating mean value, is well chosen, the result is accurate. That is, the result is accurate when the first random selected points are from first answer label group, the second random selected points are from second answer label group and the third random selected points are from third answer label group.

	\clearpage
	\item plot the log-likelihood function over the iterations! What is the behavior of this function over the iterations?

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{log_likelihood.png}
			\caption{The log-likelihood function over iterations}
		\end{center}
	\end{figure}
	As shown in Figure 4, the log-likelihood increases over iterations. That is, likelihood increased over iterations. And about 50th iteration, the function looks convering to the value, -112.96270776709655. Therfore, the process stops even though it didn't reach the max iteration number.\\

	\item Make a scatter plot of the data that shows the result of the soft-classiﬁcation that is done in the E-step

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.3\textwidth]{3class.png}
			\caption{The EM algorithm soft-classification}
			\includegraphics[width=0.3\textwidth]{answer.png}
			\caption{The answer classification}
		\end{center}
	\end{figure}

	The EM algorithm classifies well the points when it is compared with the answer classification. EM algorithm fails to classify the points near the boundary of iris-Versicolor and iris-Virgnica.
	\end{itemize}

	\subsubsection{Perform all of the above-mentioned tasks for the K-means algorithm}
	The algorithm for K-means initialization selects N different points at random from the dataset as the starting positions for N clusters.  This ensures that all centers classify at least one point during the first iteration, producing better results.
	\begin{itemize}
	\item Compare the result with the labeled data set (i.e., consider labels as well). Make a scatter plot of the data and plot the Gaussian mixture model over this plot.

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{answer.png}
			\caption{Original Data Labels}
		\end{center}
	\end{figure}

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{kmeans_cluster.png}
			\caption{K-means clustering with 3 centers}
		\end{center}
	\end{figure}

	Ultimately the K-means algorithm misclassified 13 samples, considerably worse than the EM algorithm, which we hypothesize is largely due to the limited explanatory power of the K-means approach.  Which is to say, since K-means classifies solely on euclidean distance, it lacks the finesse of the EM algorithm, which can employ a covariance matrix to describe more complex structures.

	\clearpage
	\item For your tests, select the correct number of components (K = 3), but also check the result when you use more or less components. How do you choose your initialization $\theta$0? Does this choice have an inﬂuence on the result

	The following image depicts a K-means clustering with 2 centers.
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.4\textwidth]{kmeans_2centers.png}
			\caption{K $=$ 2}
		\end{center}
	\end{figure}

	With 4 centers, K-means produces the following.
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.4\textwidth]{kmeans_4centers.png}
			\caption{K $=$ 4}
		\end{center}
	\end{figure}

	The result of more or fewer clusters than 3, while clearly not ideal, is nonetheless fairly logical.  One can see that, as more clusters are added, what was originally a single cluster begins to be broken up into smaller, similarly sized and shaped clusters.  I also believe that K-means is a better algorithm for when distances between clusters are larger, since the algorithm seems to struggle especially with tightly packed clusters.

	For initialization $\theta_0$, we selected at random as many points from the dataset as we needed centers, and used those selected points as the starting centers.  This has the beneficial for several reasons: first, it scales without modification for any number of centers less than the number of points, and second, it does not require the programmer to have any prior knowledge of the dataset they are analyzing.  This approach also ensures that every cluster is responsible for at least one point during the first iteration.

	\clearpage
	\item plot the cumulative distance over the iterations! What is the behavior of this function over the iterations?

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{kmeans_convergence.png}
			\caption{The cumulative distance over iterations}
		\end{center}
	\end{figure}
	The cumulative distance sharply decreases as the algorithm starts, indicating a quickly improving result, and shortly converges onto a solution.  Curiously, the cumulative distance does occasionally increase, indicating that across some iterations the result actually becomes slightly worse.\\

	\end{itemize}

	\clearpage

	\subsubsection{You may additionally choose any other pair of features; how would this change the classiﬁcation accuracy}

	The following figures are the scatter plots when two of four data columns are selected.
	\begin{figure}[h!]
		\centering
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{12.png}
		\end{minipage}
		\hspace{2cm}
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{13.png}
		\end{minipage}
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{14.png}
		\end{minipage}
		\hspace{2cm}
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{23.png}
		\end{minipage}
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{24.png}
		\end{minipage}
		\hspace{2cm}
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{34.png}
		\end{minipage}
	\end{figure}

	As shown in above figures, when first and third column are selected, we can draw a line to seperate the Iris-Virgnica and the Iris-Versicolor. That is, it is easy to differentiate these two groups. When other columns are selected, these two groups are mixed together. Therfore, EM algorithm also can not classify these two groups as different groups. For example, the worst one is when first and second columns are selected. When first and second columns are selected, the result is the following graph.

	\clearpage
	\begin{figure}[h!]
		\centering
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{12.png}
			\caption{The answer classfication}
		\end{minipage}
		\hspace{2cm}
		\begin{minipage}[t]{6.5cm}
			\includegraphics[width=1.0\textwidth]{12_p.png}
			\caption{EM algorithm classification}
		\end{minipage}
	\end{figure}

	As expected, the EM alogirthm result is not classified well when first and second columns are selected. That is, the EM algorithm accuracy decreased.\\
	Therfore, the column selection influences the EM algorithm accuracy.


	\subsection{4 dimensional feature}
	\subsubsection{EM algorithm tasks}
	\begin{itemize}
		\item Compare the result with the labeled data set (i.e., consider labels as well). Make a scatter plot of the data and plot the Gaussian mixture model over this plot.

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.5\textwidth]{4_number_diff.png}
				\caption{label data}
			\end{center}
		\end{figure}
		First array is EM algorithm classification. Second array is answer classification.\\
		Four points are mis-classified.

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.5\textwidth]{4_gauss.png}
				\caption{Three gaussian with scatter data points}
			\end{center}
		\end{figure}

		EM algorithm suceeded to find three gaussian model and classified the data points well.
		\clearpage
		\item For your tests, select the correct number of components (K = 3), but also check the result when you use more or less components. How do you choose your initialization $\theta$0? Does this choice have an inﬂuence on the result
		When the number of components is 2, the result is the following figure.

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.4\textwidth]{4_K2.png}
				\caption{K $=$ 2}
			\end{center}
		\end{figure}

		When the number of components is 4, the result is the following figure.
		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.4\textwidth]{4_K4.png}
				\caption{K $=$ 4}
			\end{center}
		\end{figure}

		Initalizaiton $\theta$0 is the same with the dimension 2.\\
		As written in section 1.1.1, the initialization process affects the result.

		\item plot the log-likelihood function over the iterations! What is the behavior of this function over the iterations?

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.4\textwidth]{4_log_likelihood.png}
				\caption{The log-likelihood function over iterations}
			\end{center}
		\end{figure}
		As shown in Figure 4, the log-likelihood increases over iterations. That is, likelihood increased over iterations. And about 25th iteration, the function looks convering to the value, -87.41158191236445.. Therfore, the process stops even though it didn't reach the max iteration number.\\

		\clearpage
		\item Make a scatter plot of the data that shows the result of the soft-classiﬁcation that is done in the E-step

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.3\textwidth]{4class.png}
				\caption{The EM algorithm soft-classification}
				\includegraphics[width=0.3\textwidth]{4_answer.png}
				\caption{The answer classification}
			\end{center}
		\end{figure}

		The EM algorithm classifies well the points when it is compared with the answer classification. EM algorithm fails to classify the points near the boundary of iris-Versicolor and iris-Virgnica.

	\end{itemize}

	\subsubsection{K-means algorithm tasks}
	\begin{itemize}
		\item Compare the result with the labeled data set (i.e., consider labels as well). Make a scatter plot of the data.

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{4dim_data.png}
			\caption{Original Data Labels}
		\end{center}
	\end{figure}

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.5\textwidth]{kmeans_4dim.png}
			\caption{K-means clustering with 3 centers}
		\end{center}
	\end{figure}

	In 4 dimensions, the K-means algorithm misclassified 17 samples, which is disappointingly 4 more than in 2 dimensions.  We take this to mean that while generally having more features to operate on should improve clustering accuracy, this specific dataset may not be properly suited to K-means classification, i.e. having irregular cluster shapes.

\clearpage

		\item For your tests, select the correct number of components (K = 3), but also check the result when you use more or less components. How do you choose your initialization $\theta$0? Does this choice have an inﬂuence on the result
		The following image depicts a K-means clustering with 2 centers.
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.4\textwidth]{kmeans_4dim_K2.png}
			\caption{K $=$ 2}
		\end{center}
	\end{figure}

	With 4 centers, K-means produces the following.
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.4\textwidth]{kmeans_4dim_K4.png}
			\caption{K $=$ 4}
		\end{center}
	\end{figure}

	Similar trends are seen in 4 dimensions as exist in 2 dimensions.

		\item plot the cumulative distance over the iterations! What is the behavior of this function over the iterations?

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.4\textwidth]{kmeans_4dim_conv.png}
				\caption{The cumulative distance over iterations}
			\end{center}
		\end{figure}
		This plot shows similar trends as with 2 dimensions.  The algorithm quickly converges to a solution, after which it cannot improve any further.\\


	\end{itemize}

\clearpage

	\subsubsection{How do the convergence properties and the accuracy of you classification change in comparison to scenario 2.1? }
	\begin{itemize}
		\item EM-algorithm \\
		The convergence value of log likelihood function increased. In the scenario 2.1, the value was -112.96. However, in the scenario 2.2, the value increased to -87.41. Also, the total iteration decreased in the scenario 2.2. The value decreased from 75 to 26.\\
		Also, we compared the scenario 2.1 and 2.2 with each 10 trials. Overall, the EM classifies the data points better in 2.2 scenario. The accuarcy of 2.2 scenario is higher than that of 2.1 scenario. Because the EM algorithm in 2.2 scenario has more additional information about the dataset.

		\item K-means algorithm \\
		Cumulative distance was about 20 higher in scenario 2.2 versus 2.1, while the clustering was more accurate for scenario 2.2.  This can be explained by the fact that with euclidean distances, more dimensions will generally increase distance.  Nonetheless, the additional insight gained by clustering with all 4 features significantly improved our results.

	\end{itemize}

	\clearpage
	\subsubsection{Within your EM-function confine the structure of the covariance matrices to diagonal matrices! What is the inﬂuence on the result.}
	we confined the structure of the covariance matrices to diagonal matrices by setting non-diagonal matrices to 0. These following figures are the EM results with diagonal covariance matrices.

	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.3\textwidth]{4_answer.png}
			\caption{The answer classification}
			\includegraphics[width=0.3\textwidth]{4class.png}
			\caption{The EM algorithm soft-classification}
			\includegraphics[width=0.3\textwidth]{4_cov0.png}
			\caption{The EM algorithm soft-classification with diagonal covariance}
		\end{center}
	\end{figure}

	As shown in the figure 23,24,25, the result with diagonal covariacne, looks similar to th others. That is, the diagonal convariance doesn't influence on the result.
        \clearpage
	\subsection{Processing the data with PCA }
	\subsubsection{How much of the variance in the data is explained this way?}
        \begin{itemize}
          \item original variance(sum of eigenvalues) -  4.499157046979866
          \item associated eigenvalues - 4.15886089, 0.23573307
          \item the amount of explained variance - 0.9767594057574307
        \end{itemize}
        \subsubsection{How does the performance of your algorithms compare to scenario 2.1 and scenario 2.2?}

          Scenario 2.1 has no difference at performance for both algorithms. Actually it takes more time to do PCA.
          The amount of explained variance is 1. The data from PCA is rotated original data that eigenvectors are the axes.
          The numbers of miscounted samples were 3 with EM and 13 with K-means. Overall, EM and K-means, both showed similar behavior.

          \begin{figure}[h!]
            \centering
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_1_ans.png}
              \caption{answer for pca scenario 1}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_1_1.png}
              \caption{scatter plot for scenario 1. EM}
            \end{minipage}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_1_2.png}
              \caption{log-likelihood plot for scenario 1. EM}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_1_3.png}
              \caption{scatter plot with Gaussian for scenario 1. EM}
            \end{minipage}
          \end{figure}
          \clearpage
          \begin{figure}[h!]
            \centering
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_s1_2.png}
              \caption{scatter plot for scenario 1. EM. K=2}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_s1_4.png}
              \caption{scatter plot for scenario 1. EM. K=4}
            \end{minipage}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_k_s1_2.png}
              \caption{scatter plot for scenario 1. K-means. K=2}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_k_s1_4.png}
              \caption{scatter plot for scenario 1. K-means. K=4}
            \end{minipage}
          \end{figure}

        EM showed better performance in scenario 2.2. PCA reduced the dimension of data
        to 2 so it took less time and showed more accuracy. The number of misclassified sample
        reduced from 4 to 3 with PCA. K-means showed significant difference in its accuracy.
        Without PCA, the number of misclassified sample was 17 and it became 9 with PCA.

        \begin{figure}[h!]
            \centering
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_2_ans.png}
              \caption{answer for pca scenario 2}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_2_1.png}
              \caption{scatter plot for scenario 2. EM}
            \end{minipage}
        \end{figure}
        \clearpage
        \begin{figure}[h!]
            \centering
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_2_2.png}
              \caption{log-likelihood plot for scenario 2. EM}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_2_3.png}
              \caption{scatter plot with Gaussian for scenario 2. EM}
            \end{minipage}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_k_2_1.png}
              \caption{scatter plot for scenario 2. K-means}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_k_2_2.png}
              \caption{cumulative distance for k-means plot for scenario 2. K-means}
            \end{minipage}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_s2_2.png}
              \caption{scatter plot for scenario 2. EM. K=2}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_em_s2_4.png}
              \caption{scatter plot for scenario 2. EM. K=4}
            \end{minipage}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_k_s2_2.png}
              \caption{scatter plot for scenario 2. K-means. K=2}
            \end{minipage}
            \hspace{2cm}
            \begin{minipage}[t]{6.5cm}
              \includegraphics[width=1.0\textwidth]{pca_k_s2_4.png}
              \caption{scatter plot for scenario 2. K-means. K=4}
            \end{minipage}
        \end{figure}
        \clearpage
	\subsubsection{Apply PCA with whitening, so that the transformed data has zero mean and a unit covariance matrix. How does this influence the choice of your initialization?}
        Compared PCA without whitening and PCA with whitening on scenario 2(4 dimension $\rightarrow$ 2 dimension), initialization result is below.
        \begin{itemize}
          \item EM
            \begin{itemize}
              \item without whitening
                \begin{itemize}
                  \item $\alpha$ \\ $ \begin{bmatrix}
                      0.33333333 & 0.33333333 & 0.33333333
                  \end{bmatrix}  $
                \item mean \\ $ \begin{bmatrix}
                    1.13021491 & -0.01001823 & 1.13587265 \\
                    0.23478179 & 0.11992288 & -0.42093766
                \end{bmatrix}  $
              \item covariance \\ $ \begin{bmatrix}
                  4.15886089e+00 & 4.15886089e+00 & 4.15886089e+00 \\
                  -8.58001217e-16 & -8.58001217e-16 & -8.58001217e-16
              \end{bmatrix}  $ \\ $ \begin{bmatrix}
                -8.58001217e-16 & -8.58001217e-16 & -8.58001217e-16 \\
                2.35733074e-01 & 2.35733074e-01 & 2.35733074e-01
              \end{bmatrix}  $
                \end{itemize}
              \item with whitening
                \begin{itemize}
                  \item $\alpha$ \\ $ \begin{bmatrix}
                      0.33333333 & 0.33333333 & 0.33333333
                  \end{bmatrix}  $
                \item mean \\ $ \begin{bmatrix}
                    0.65215026 & -2.61884422 & -2.74090941 \\
                    -0.1535649 & -0.0246702 & -0.2064007
                \end{bmatrix}  $
              \item covariance \\ $ \begin{bmatrix}
                  1.72961239e+01 & 1.72961239e+01 & 1.72961239e+01 \\
                  -1.05433932e-15 & -1.05433932e-15 & -1.05433932e-15
              \end{bmatrix}  $ \\ $ \begin{bmatrix}
                -1.05433932e-15 & -1.05433932e-15 & -1.05433932e-15 \\
                5.55700821e-02 & 5.55700821e-02 & 5.55700821e-02
              \end{bmatrix}  $
                \end{itemize}
            \end{itemize}
            The mean of sample means became close to 0 after whitening, assumed that
            the number of choosen samples in each class is same. The covariances
            also changed. Theoritically, the covariance matrix with 4 dimension should
            be identity matrix. However, our covariances is not covariance matrix with 4 data dimension.
            Whitening influenced somehow to initialize EM.
          \item K-means
            \begin{itemize}
              \item without whitening - initial values \\ $ \begin{bmatrix}
                  0.44172808 & 1.97553771 & -2.56665681 \\
                  0.44933724 & -0.0330603 & -0.31820248
              \end{bmatrix} $
              \item with whitening - initial values \\ $ \begin{bmatrix}
                -5.54268692 & 1.16714556 & -4.69918262 \\
                0.108882 &  -0.08323771 & -0.65221037
              \end{bmatrix} $
            \end{itemize}
            Calcutated mean value of initial values in each dimension became close to 0 after whitening.
        \end{itemize}

        \clearpage
	\section{Samples from a Gaussian Mixture Model}
	\subsection{Write a function Y = sample-GMM(alpha, mu, cov, N)}

	Our algorithm works first by constructing a discrete distribution from the alpha values of each Gaussian Distribution, then using this discrete distribution to assign each sample to a Gaussian Distribution, and finally placing the sample according to its assigned Gaussian Distribution.  The probability of each Gaussian Distribution being selected for a new sample is equal to its corresponding alpha value.

	\subsection{Using a GMM of your choice$ (K > 3)$, demonstrate the correctness of your function}

	\begin{figure}[h]
			\begin{center}
				\includegraphics[width=0.4\textwidth]{GMM_samples.png}
				\caption{100 Samples drawn from a Gaussian Mixture Model}
			\end{center}
		\end{figure}

	This result demonstrates the correctness of our algorithm.  For parameters we used the following alpha values: [0.05, 0.2, 0.3, 0.45] and following mean values: [(4, 4), (-4, 4), (0, 0), (-4, -6)].  You can see that the Gaussian Distribution centered at (4, 4) has a very low alpha (0.05), since it has so few points compared to the others.  The covariance matrices are as follows.

	\[
	\Sigma_1 =
 	\begin{bmatrix}
  	.2 & 0 \\
  	0 & .2
 	\end{bmatrix}
 	\]

	\[
	\Sigma_2 =
 	\begin{bmatrix}
  	.3 & 0 \\
  	0 & .3
 	\end{bmatrix}
 	\]

	\[
	\Sigma_3 =
 	\begin{bmatrix}
  	.3 & .25 \\
  	.25 & .3
 	\end{bmatrix}
 	\]

 	\[
 	\Sigma_4 =
 	\begin{bmatrix}
  	1 & 0 \\
  	0 & .1
 	\end{bmatrix}
	\]

\end{document}
